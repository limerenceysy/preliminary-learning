{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f6496f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "#实现输入x与核矩阵k的互相关运算\n",
    "def corr2d(X,K):\n",
    "    h,w=K.shape\n",
    "    Y=torch.zeros((X.shape[0]-h+1,X.shape[1]-w+1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i,j]=(X[i:i+h,j:j+w]*K).sum()\n",
    "    return Y\n",
    "\n",
    "#verification\n",
    "X=torch.arange(9).reshape(3,3)\n",
    "K=torch.tensor([[0,1],[2,3]])\n",
    "corr2d(X,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd23e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 0., 1., 1.]]),\n",
       " tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "         [ 0.,  1.,  0.,  0.,  0., -1.,  0.]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#自己实现一个卷积层\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight=nn.parameter(torch.rand(kernel_size))\n",
    "        self.bias=nn.parameter(torch.zeros(1))\n",
    "    def forward(self,x):\n",
    "        return corr2d(x,self.weight)+self.bias\n",
    "#使用nn内置的卷积层，手动完成一个学习卷积核的过程\n",
    "X=torch.ones((6,8))\n",
    "X[:,2:6]=0\n",
    "K=torch.tensor([[1,-1]])\n",
    "Y=corr2d(X,K)\n",
    "X,Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6056e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2,loss 11.105\n",
      "epoch 4,loss 2.818\n",
      "epoch 6,loss 0.864\n",
      "epoch 8,loss 0.305\n",
      "epoch 10,loss 0.117\n"
     ]
    }
   ],
   "source": [
    "con_net=nn.Conv2d(1,1,kernel_size=(1,2),bias=False)\n",
    "X=X.reshape((1,1,6,8))\n",
    "Y=Y.reshape((1,1,6,7))\n",
    "lr=3e-2\n",
    "for i in range(10):\n",
    "    Y_hat=con_net(X)\n",
    "    l=(Y_hat-Y)**2\n",
    "    con_net.zero_grad()\n",
    "    l.sum().backward()\n",
    "    con_net.weight.data[:]-=lr*con_net.weight.grad\n",
    "    if(i+1)%2==0:\n",
    "        print(f'epoch {i+1},loss {l.sum():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf5c6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9517, -1.0205]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_net.weight.data.reshape((1,2))\n",
    "#如果不加reshape的话，为什么嵌套层数那么多？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85debab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#多输入通道互相关\n",
    "def corr2d_multi_in(X,K):\n",
    "    return sum(d2l.corr2d(x,k) for x,k in zip(X,K))#无非是每个通道依次和对应的输入矩阵进行互相关运算\n",
    "\n",
    "X1=torch.tensor([[[0,1,2],[3,4,5],[6,7,8]],[[1,2,3],[4,5,6],[7,8,9]]])\n",
    "K1=torch.tensor([[[0,1],[2,3]],[[1,2],[3,4]]])\n",
    "corr2d_multi_in(X1,K1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d5bc407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#多输出通道互相关\n",
    "def corr2d_multi_in_out(X,K):#K此时是一个4维的，最外面的维度是Cout\n",
    "    return torch.stack([corr2d_multi_in(X,k) for k in K],0) #torch.stack用于将多个输出通道的结果堆叠起来。\n",
    "K2=torch.stack((K1,K1+1,K1+2),0) #K2.size为3*2*2*2,因为K1.size为2*2*2\n",
    "corr2d_multi_in_out(X1,K2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f4dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pooling layer的前向传播\n",
    "def pool2d(X,pool_size,mode='max'):\n",
    "    p_h,p_w=pool_size\n",
    "    Y=torch.zeros((X.shape[0]-p_h+1,X.shape[1]-p_w+1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode=='max':\n",
    "                Y[i,j]=X[i:i+p_h,j:j+p_w].max()\n",
    "            elif mode=='avg':\n",
    "                Y[i,j]=X[i:i+p_h,j:j+p_w].mean()\n",
    "    return Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbabe1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12],\n",
      "        [13, 14, 15],\n",
      "        [16, 17, 18]])\n",
      "tensor([[ 1,  2,  3,  7,  8,  9, 13, 14, 15],\n",
      "        [ 4,  5,  6, 10, 11, 12, 16, 17, 18]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "c = torch.tensor([[13, 14, 15], [16, 17, 18]])\n",
    "result1 = torch.cat([a, b, c], dim=0) #6*3!!第 0 维度的大小是各个输入张量在第 0 维度上的大小之和\n",
    "print(result1)\n",
    "result2 = torch.cat([a, b, c], dim=1)#横着拼，增加列数\n",
    "print(result2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
